# JDCrawler (Job Description Crawler)

í•œêµ­ ì£¼ìš” ì±„ìš© ì‚¬ì´íŠ¸(Jobkorea, Saramin, Wanted)ì˜ ì±„ìš© ê³µê³ ë¥¼ ìˆ˜ì§‘í•˜ê³ , AIë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ë§ì¶¤í˜•ìœ¼ë¡œ ë¶„ì„í•´ì£¼ëŠ” í†µí•© ëŒ€ì‹œë³´ë“œ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

## ğŸš€ Key Features

### 1. Multi-Site Crawling
- **ì§€ì› ì‚¬ì´íŠ¸**: ì¡ì½”ë¦¬ì•„, ì‚¬ëŒì¸, ì›í‹°ë“œ
- **ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘**: Playwrightë¥¼ í™œìš©í•œ ë™ì  í˜ì´ì§€ í¬ë¡¤ë§ ë° ë¡œë´‡ íƒì§€ ìš°íšŒ
- **ì¤‘ë³µ ì œê±°**: RapidFuzzë¥¼ ì´ìš©í•œ ìœ ì‚¬ ê³µê³  í•„í„°ë§ ë° í†µí•©

### 2. AI-Powered Analysis
- **ìë™ ë¶„ì„**: ìˆ˜ì§‘ëœ ê³µê³ ë¥¼ AIê°€ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìš”ì•½ ë° í‰ê°€
- **ë§ì¶¤í˜• ì ìˆ˜**: ì‚¬ìš©ì í”„ë¡œí•„(ê¸°ìˆ  ìŠ¤íƒ, ì œì™¸ í‚¤ì›Œë“œ ë“±) ê¸°ë°˜ ì í•©ë„ ì ìˆ˜ ì‚°ì¶œ
- **ìŠ¤í‚¬ ë§¤ì¹­**: ê³µê³  ë‚´ ìš”êµ¬ ìŠ¤í‚¬ê³¼ ì‚¬ìš©ì ë³´ìœ  ìŠ¤í‚¬ ë§¤ì¹­ ì‹œê°í™”

### 3. Modern Dashboard
- **í†µí•© ë·°**: ëª¨ë“  ì‚¬ì´íŠ¸ì˜ ê³µê³ ë¥¼ í•œê³³ì—ì„œ ê²€ìƒ‰ ë° í•„í„°ë§
- **ë°ì´í„° ì‹œê°í™”**: ì¼ë³„ ìˆ˜ì§‘ í˜„í™©, ì‚¬ì´íŠ¸ë³„ ë¶„í¬, í¬ì§€ì…˜ ë¶„ì„ ì°¨íŠ¸ ì œê³µ
- **í‚¤ì›Œë“œ ê´€ë¦¬**: ìˆ˜ì§‘ ëŒ€ìƒ í‚¤ì›Œë“œ ë° í™œì„±/ë¹„í™œì„± ìƒíƒœ ê´€ë¦¬

## ğŸ›  Tech Stack

### Backend
- **Framework**: Python 3.11+, FastAPI
- **Crawling**: Playwright, BeautifulSoup4
- **Database**: SQLite (SQLAlchemy + Pydantic)
- **Task Queue**: APScheduler (Periodic Crawling)
- **AI/ML**: RapidFuzz (Duplicate Detection)

### Frontend
- **Framework**: React 19, Vite
- **Language**: TypeScript
- **Styling**: Tailwind CSS v4, shadcn/ui
- **State Management**: React Query (@tanstack/react-query)
- **Visualization**: Recharts

## ğŸ Quick Start

### Prerequisites
- Python 3.11 or higher
- Node.js 18 or higher
- pnpm (recommended) or npm

### 1. Clone Repository
```bash
git clone https://github.com/cmgeugene/JDCrawler.git
cd JDCrawler
```

### 2. Backend Setup
```bash
cd backend

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"
playwright install chromium

# Environment Setup
cp .env.example .env
# Edit .env with your API keys (ZHIPU_API_KEY etc.)

# Run Server
./run_backend.sh
# Or manually: uvicorn jdcrawler.main:app --reload
```

### 4. Docker Deployment (Recommended for Production)

ë„ì»¤ë¥¼ ì´ìš©í•˜ë©´ ë³µì¡í•œ ì„¤ì¹˜ ê³¼ì • ì—†ì´ ë°±ì—”ë“œì™€ í”„ë¡ íŠ¸ì—”ë“œë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì„ ì—´ì–´ ZHIPU_API_KEY ë“±ì„ ì‹¤ì œ í‚¤ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.

# 2. ì„œë¹„ìŠ¤ ì‹¤í–‰
docker-compose up -d --build
```
ì‹¤í–‰ í›„ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost`ì— ì ‘ì†í•˜ì„¸ìš”.

## ğŸ“ Project Structure

```
JDCrawler/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ jdcrawler/
â”‚   â”‚   â”œâ”€â”€ api/          # FastAPI endpoints (jobs, crawl, keywords, etc.)
â”‚   â”‚   â”œâ”€â”€ crawlers/     # Site-specific crawler implementations
â”‚   â”‚   â”œâ”€â”€ db/           # Database schema, client, and migrations
â”‚   â”‚   â”œâ”€â”€ models/       # Pydantic data models for API & internal use
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic (crawler orchestration, AI analysis)
â”‚   â”‚   â””â”€â”€ utils/        # Shared utilities (rate limiting, retries)
â”‚   â””â”€â”€ tests/            # Unit, integration, and E2E tests
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # UI components (jobs, layout, common ui)
â”‚   â”‚   â”œâ”€â”€ lib/          # API client (Axios) and utility functions
â”‚   â”‚   â”œâ”€â”€ pages/        # Main route pages (Dashboard, Jobs, etc.)
â”‚   â”‚   â”œâ”€â”€ queries/      # React Query hooks for data fetching
â”‚   â”‚   â””â”€â”€ types/        # TypeScript interfaces and types
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## ğŸ“ License

This project is licensed under the MIT License.